% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wrf.R
\name{wRandomforest}
\alias{wRandomforest}
\title{Weighted Random Forest prediction models for complex survey data}
\usage{
wRandomforest(
  data = NULL,
  col.x = NULL,
  y = NULL,
  xtest = NULL,
  ytest = NULL,
  cluster = NULL,
  strata = NULL,
  weights = NULL,
  design = NULL,
  method = c("dCV", "JKn", "bootstrap", "subbootstrap", "BRR", "split", "extrapolation"),
  k = 10,
  R = 1,
  B = 200,
  dCV.sw.test = FALSE,
  train.prob = 0.7,
  method.split = c("dCV", "bootstrap", "subbootstrap"),
  print.rw = FALSE,
  ntree = 500,
  mtry = if (!is.null(y) && !is.factor(y)) max(floor(ncol(data[, col.x])/3), 1) else
    floor(sqrt(ncol(data[, col.x]))),
  replace = TRUE,
  classwt = NULL,
  cutoff,
  sampsize = if (replace) nrow(data) else ceiling(0.632 * nrow(data)),
  nodesize = if (!is.null(y) && !is.factor(y)) 5 else 1,
  maxnodes = NULL,
  importance = FALSE,
  proximity = FALSE,
  oob.prox = proximity,
  norm.votes = TRUE,
  do.trace = FALSE,
  keep.forest = TRUE,
  corr.bias = FALSE,
  keep.inbag = FALSE,
  ...
)
}
\arguments{
\item{data}{A data frame with information about independent variables, as well as sampling weights and strata and cluster indicators. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.}

\item{col.x}{A numeric vector indicating indices of columns for covariates or independent variables or a vector of character strings indicating names of these columns.}

\item{y}{A vector of the response variable. If a factor, classification is assumed, otherwise, regression is assumed. If omitted, randomForest will run in a unsupervised mode.}

\item{xtest}{A data frame or matrix containing predictors for the test set.}

\item{ytest}{A vector of response variable for the test set.}

\item{cluster}{A character string indicating the name of cluster identifiers. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.}

\item{strata}{A character string indicating the name of strata identifiers. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.}

\item{weights}{A character string indicating the name of sampling weights. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.}

\item{design}{An object of class \code{survey.design} generated by \code{survey::svydesign()}. It could be \code{NULL} if information about \code{cluster}, \code{strata}, \code{weights} and \code{data} were given.}

\item{method}{A character string indicating a method of replicate weights. Choose one of these: \code{JKn}, \code{dCV}, \code{bootstrap}, \code{subbootstrap}, \code{BRR}, \code{split}, \code{extrapolation}.}

\item{k}{An integer. The number of folds for the \code{dCV} method. Default is \code{k=10}.}

\item{R}{An integer. The number of times the sample is partitioned for \code{dCV}, \code{split} or \code{extrapolation} method. Default is \code{R=1}.}

\item{B}{An integer. The number of bootstrap re-samples for \code{bootstrap} and \code{subbootstrap} methods. Default is \code{B=200}.}

\item{dCV.sw.test}{A logical value indicating the method for estimating the error for \code{dCV} method. \code{FALSE}, (the default option) estimates the error for each test set and defines the cross-validated error based on the average strategy. Option \code{TRUE} estimates the cross-validated error based on the pooling strategy}

\item{train.prob}{A numeric between 0 and 1, indicating the proportion of clusters (for the method \code{split}) or strata (for the method \code{extrapolation}) to be set in the training sets. Default is \code{train.prob = 0.7}. Only applies for \code{split} and \code{extrapolation} methods.}

\item{method.split}{A string of one the following replicate weights methods to be implemented under the \code{split} method in the \code{method} argument: \code{dCV}, \code{bootstrap} or \code{subbootstrap}.}

\item{print.rw}{A logical value. If \code{TRUE}, the data set with the replicate weights is saved in the output object. Default \code{print.rw=FALSE}.}

\item{ntree}{The number of trees to grow. This should not set to be too small a number, to ensure that every input row gets predicted at least a few times.}

\item{mtry}{The number of variables randomly sampled as candidates at each split. Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3)}

\item{replace, classwt, cutoff, sampsize, nodesize, maxnodes, importance, proximity, oob.prox, norm.votes, do.trace, keep.forest, corr.bias, keep.inbag, ...}{Optional parameters to be passed to the low level function \code{\link[randomForest:randomForest]{randomForest::randomForest()}}.}
}
\value{
The output object of the function \code{wRandomforest()} is an object of class \code{w.randomforest}:
\itemize{
\item \code{mtry}: A list containing information on \code{mtry} as a tuning parameter (the number of predictors sampled for splitting at each node):
\itemize{
\item \code{all}: A numeric vector of \code{k*R} \code{mtry}- values, i.e. \code{mtry} for each fold per replicate.
\item \code{optimal.mtry}: An optimal tuning parameter, \code{mtry} among \code{mtry$all} that minimizes the average error.
}
\item \code{evaluation_log}: A list containing information of two elements:
\itemize{
\item \code{weighted.test.error}: A vector of the average errors corresponding to tuning parameters in \code{mtry$all}.
\item \code{min.weighted.test.error}: An minimum error for the optimal tuning parameter from \code{mtry$optimal.mtry} to select the final model.
}
\item \code{model}: A list containing information on the fitted model by \code{optimal.mtry}: an object of class \code{\link[randomForest]{randomForest}}.
Note that the selected model is fitted by the original \code{data}, not one of the generated training sets by replicate weights.
\item \code{data.rw}: A data frame containing the original data set and the replicate weights added to define training and test sets. Only included in the output object if \code{print.rw=TRUE}.
\item \code{call}: The call that executes this function producing the object of \code{w.randomforest}.
}
}
\description{
A function to fit wRF prediction (linear or logistic) models for complex survey data with sampling weights in the estimation process and
to select tuning parameters minimizing the weighted error for a selected replicating weights method.  Detailed arguments are referred to in R-\code{\link{randomForest}}.
}
\note{
\code{final.model} can leverage R-\code{randomForest} functionality for comprehensive data analysis for linear/logistic regression but will be
extensively used for other models, i.e., Cox, Poisson, etc.
}
\examples{
 # Global options to avoid only one PSU in a stratum in a particular domain or subpopulation
options(survey.adjust.domain.lonely=TRUE)
options(survey.lonely.psu="adjust")


 # For classification: y should be a factor.
 # If a test set is provided, one can plug it into `xtest`, `ytest`
 # just as implemented in \code{randomForest::randomForest()}.

 data(nhanes2013_sbc)
dcv.rf <- wRandomforest(data = nhanes2013_sbc,
              y = as.factor(nhanes2013_sbc$HBP), col.x = 2:61,
              cluster = "SDMVPSU", strata = "SDMVSTRA", weights = "WTSAF2YR",
              method = "dCV", k=5, R=5,importance=TRUE,proximity = TRUE)

# Or equivalently:
des <- survey::svydesign(ids=~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTSAF2YR,
                              nest = TRUE, data =nhanes2013_sbc)
dcv.rf <- wRandomforest(y = as.factor(nhanes2013_sbc$HBP), col.x = 2:61, design =des,
                        importance=TRUE,proximity = TRUE,
                        method = "dCV", k=5, R=5)

}
\seealso{
\code{\link[randomForest:randomForest]{randomForest::randomForest()}} for arguments and return values in detail.
}
