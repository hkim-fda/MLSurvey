% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wxgboost.R
\name{wXGBoost}
\alias{wXGBoost}
\title{Weighted XGBoost prediction models for complex survey data}
\usage{
wXGBoost(
  data = NULL,
  y = NULL,
  col.x = NULL,
  missing = NA,
  cluster = NULL,
  strata = NULL,
  weights = NULL,
  design = NULL,
  params = list(),
  nrounds,
  verbose = 0,
  print_every_n = 1L,
  maximize = FALSE,
  early_stopping_rounds = 8,
  final.model = TRUE,
  method = c("dCV", "JKn", "bootstrap", "subbootstrap", "BRR", "split", "extrapolation"),
  k = 10,
  R = 1,
  B = 200,
  dCV.sw.test = FALSE,
  feval = NULL,
  train.prob = 0.7,
  method.split = c("dCV", "bootstrap", "subbootstrap"),
  print.rw = FALSE,
  save_period = NULL,
  save_name = "wxgboost.model",
  wxgb_model = NULL,
  callbacks = list(),
  ...
)
}
\arguments{
\item{data}{A data frame with information about the response variable and covariates, as well as sampling weights, strata, and cluster indicators. It could be \code{NULL} if the sampling design were plugged into the \code{design} argument.}

\item{y}{A vector of response variable.}

\item{col.x}{A numeric vector indicating indices of the covariates or a string vector indicating these column names.}

\item{missing}{Default is set to \code{NA}, which means that \code{NA} values should be considered as 'missing' by the algorithm.  Sometimes, \code{0} or other extreme values might be used to represent missing values. This parameter is only used when input is a dense matrix.}

\item{cluster}{A string of a cluster identifier. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.}

\item{strata}{A string of a strata identifiers. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.}

\item{weights}{A string of a sampling weights' identifier. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.}

\item{design}{An object of class \code{survey.design} generated by \code{survey::svydesign()}. It could be \code{NULL} if information about \code{cluster}, \code{strata}, \code{weights} and \code{data} were given.}

\item{params}{The list of parameters.  Should include \code{objective} to be a string of either \code{binary:logistic} for binary classification or \code{reg:squarederror} for linear regression.
The complete list of parameters is found on \code{\link[xgboost:xgb.train]{xgboost::xgb.train()}} or \url{https://xgboost.readthedocs.io/en/latest/parameter.html}.}

\item{nrounds}{The maximum number of boosting iterations.}

\item{verbose}{\code{0} for xgboost to stay silent.  Default= \code{1} for printing information about performance. \code{2} for printing some additional information out. Note that setting verbose > 0 automatically engages the \code{cb.print.evaluation}(period=1) callback function.}

\item{print_every_n}{Print each n-th iteration evaluation messages when verbose>0. Default is \code{1} printing all messages. This parameter is passed to the \code{cb.print.evaluation} callback.}

\item{maximize}{If \code{feval} and \code{early_stopping_rounds} are set, then this parameter must be set as well. When it is \code{TRUE}, it means the larger the evaluation score the better.
This parameter is passed to the \code{\link[xgboost]{cb.early.stop}} callback.}

\item{early_stopping_rounds}{\code{NULL} does not trigger the early stopping function. Setting to an integer k will stop training with a validation set unless the performance improve for k rounds.  Setting this parameter engages the \code{cb.early.stop} callback.}

\item{final.model}{Boolean.  Default is \code{TRUE} printing out the object of class \code{xgb.Booster} for the final model selected by replicate weights' CV with corresponding predicted values.}

\item{method}{A character string indicating a method of replicate weights. Choose one of these: \code{JKn}, \code{dCV}, \code{bootstrap}, \code{subbootstrap}, \code{BRR}, \code{split}, \code{extrapolation}.}

\item{k}{An integer. The number of folds for the \code{dCV} method. Default is \code{k=10}.}

\item{R}{An integer. The number of times the sample is partitioned for \code{dCV}, \code{split} or \code{extrapolation} method. Default is \code{R=1}.}

\item{B}{An integer. The number of bootstrap re-samples for \code{bootstrap} and \code{subbootstrap} methods. Default is \code{B=200}.}

\item{dCV.sw.test}{A logical value indicating the method for estimating the error for \code{dCV} method. \code{FALSE}, (the default option) estimates the error for each test set and defines the cross-validated error based on the average strategy.
\code{TRUE} estimates the cross-validated error based on the pooling strategy.}

\item{feval}{User defined evaluation function. Returns list(metric='metric-name', value='metric-value') with given prediction and dtrain.
Default evaluation metric is assigned by \code{params:objective}: weighted mse (\code{\link[=evalerror_reg]{evalerror_reg()}}) for regression, weighted mean error (\code{\link[=evalerror_bin]{evalerror_bin()}}) for logistic reg or binary classification.}

\item{train.prob}{A numeric between 0 and 1, indicating the proportion of clusters (for the method \code{split}) or strata (for the method \code{extrapolation}) to be set in the training sets. Default is \code{train.prob = 0.7}. Only applies to \code{split} and \code{extrapolation} methods.}

\item{method.split}{A string of one the following replicate weights methods to be implemented under the \code{split} in the \code{method} argument: \code{dCV}, \code{bootstrap} or \code{subbootstrap}.}

\item{print.rw}{A logical value. If \code{TRUE}, the data set with the replicate weights is saved in the output object. Default \code{print.rw=FALSE}.}

\item{save_period}{When it is non-NULL, model is saved to disk after every \code{save_period} rounds, \code{0} means save at the end. The saving is handled by the \code{cb.save.model} callback.}

\item{save_name}{The name or path for periodically saved model file.}

\item{wxgb_model}{A previously built model to continue the training from. Could be either an object of class \code{xgb.Booster}, i.e., \code{final.model} from the object of class \code{w.xgboost}, its raw data, or the name of a file with a previously saved model.}

\item{callbacks}{A list of callback functions to perform various task during boosting. See \code{\link[xgboost]{callbacks}}. Some of the callbacks are automatically created depending on the parametersâ€™ values.
Users can provide either existing or their own callback methods in order to customize the training process.}

\item{...}{optional parameters to be passed to the low level function \code{xgboost::xgb.train()} or \code{xgboost::xgboost()}.}
}
\value{
The output object of the function \code{wXGBoost()} is an object of class \code{w.xgboost}:
\itemize{
\item \code{CV.iterations}: A list containing information on tuning parameters, \code{nrounds} of three elements:
\itemize{
\item \code{niter.range.per.R}: A matrix of the range of iterations of \code{k}-fold CV by replicate weights for \code{R} replicates
\item \code{k_fold.best}: A matrix of best iterations, minimizing the average error, for corresponding folds for \code{R} replicates.
\item \code{best_iteration}: An optimal \code{nround} that minimizes mean test error.
}
\item \code{CV.eval_log}: A list containing information of two elements:
\itemize{
\item \code{CV}: A matrix of iteration-wise mean and sd for training and test sets, respectively.
\item \code{Best_iteration}: A vector of minimum \code{CV.eval_log$CV} to pick up the final model corresponding to \code{CV.iteratons$best_iteration}.
}
\item \code{final.model}: A list containing information on the fitted model by \code{Best_iteration}: an object of class \code{xgb.Booster}.
Note that the final model is fitted by the original \code{data}, not one of the generated training sets by replicate weights.
\item \code{predicted}: A vector of predicted values from \code{final.model}.
\item \code{data.rw}: A data frame containing the original data set and the replicate weights added to define training and test sets. Only included in the output object if \code{print.rw=TRUE}.
\item \code{call}:The call that executes this function producing the object of \code{w.xgboost}.
}
}
\description{
A function to fit XGBoost prediction (linear or logistic) models for complex survey data with sampling weights in the estimation process and
to select tuning parameters by minimizing the weighted mean error for a selected replicating weights method.  Detailed arguments are referred to in R-\code{\link{xgboost}}.
}
\note{
\code{final.model} can leverage R-\code{xgboost} functionality for comprehensive data analysis for linear/logistic regression but will be
extensively used for other models, i.e., Cox, Poisson, etc.
}
\examples{

 # Global options to avoid only one PSU in a stratum in a particular domain or subpopulation
   options(survey.adjust.domain.lonely=TRUE)
   options(survey.lonely.psu="adjust")

data(nhanes2013_sbc)

# Selecting optimal (hyper-) parameters for a model development:
param <- list(objective = "binary:logistic", max_depth = sample(4:8, 1),
             eta = runif(1, .01, .3),gamma = runif(1, 0.0, 1),
             subsample = 0.5, # much smaller for large N
             colsample_bytree = runif(1, .5, .8),
             min_child_weight = sample(1:40, 1),
             max_delta_step = sample(1:10, 1))  # for very imbalanced case

# Variable selection with optimal (hyper-) parameters:
xgb.cv <- wXGBoost(data = nhanes2013_sbc,
              y = nhanes2013_sbc$HBP, col.x = 2:61,
              cluster = "SDMVPSU", strata = "SDMVSTRA", weights = "WTSAF2YR",
              params = param, nrounds =10000,verbose=0,early_stopping_rounds=5,
              method = "dCV", k=10, R=20)
\dontrun{
# Or equivalently:
des <- survey::svydesign(ids=~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTSAF2YR,
                              nest = TRUE, data = nhanes2013_sbc)
xgb.cv <- wXGBoost(y = nhanes2013_sbc$HBP, col.x = 2:61, design = des,
              params = param, nrounds =10000,verbose=0,early_stopping_rounds=5,
              method = "dCV", k=10, R=20)
              }
}
\seealso{
\code{\link[xgboost:xgb.train]{xgboost::xgb.train()}} for relevant arguments and return values in detail.

\code{\link[=eval_loss]{eval_loss()}} for additional customized function for weighted logloss.
}
