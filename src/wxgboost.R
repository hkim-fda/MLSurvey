#' Weighted Xgboost prediction models for complex survey data
#'
#'@description This function fits Xgboost prediction (linear or logistic) models to complex survey data, using sampling weights in the estimation process and 
#'             selects the tuning parameters that minimize the error based on different replicating weights methods.  Detailed arguments refer to R-\code{xgboost}.
#'
#' @param data A data frame with information about the response variable and covariates, as well as sampling weights and strata and cluster indicators. It could be \code{NULL} if the sampling design is indicated in the \code{design} argument.
#' @param y A numeric vector of response variable. 
#' @param col.x A numeric vector indicating the numbers of the columns in which information on the covariates can be found or a vector of character strings indicating the names of these columns.
#' @param cluster A character string indicating the name of the column with cluster identifiers. It could be \code{NULL} if the sampling design is indicated in the \code{design} argument.
#' @param strata A character string indicating the name of the column with strata identifiers. It could be \code{NULL} if the sampling design is indicated in the \code{design} argument.
#' @param weights A character string indicating the name of the column with sampling weights. It could be \code{NULL} if the sampling design is indicated in the \code{design} argument.
#' @param design An object of class \code{survey.design} generated by \code{survey::svydesign()}. It could be \code{NULL} if information about \code{cluster}, \code{strata}, \code{weights} and \code{data} are given.
#' @param params The list of parameters.  Should include \code{objective} to be a string of either `binary:logistic` for binary classification or `reg:squarederror` for linear regression.  
#'                The complete list of parameters is found on [xgboost::xgb.train()] or \url{https://xgboost.readthedocs.io/en/latest/parameter.html}.  
#' @param nrounds Maximum number of boosting iterations
#' @param print_every_n Print each n-th iteration evaluation messages when verbose>0. Default is `1` printing all messages. This parameter is passed to the `cb.print.evaluation` callback.
#' @param verbose  `0` for xgboost to stay silent.  Default= `1` for printing information about performance. `2` for printing some additional information out. Note that setting verbose > 0 automatically engages the `cb.print.evaluation`(period=1) callback function.
#' @param early_stopping_rounds \code{NULL} does not trigger the early stopping function. Setting to an integer k will stop training with a validation set unless the performance improve for k rounds.  Setting this parameter engages the `cb.early.stop` callback
#' @param final.model Boolean.  Default is \code{TRUE} printing out the object of class \code{xgb.Booster} for the final model selected by replicate weights' CV with corresponding predicted values.  
#' @param method A character string indicating the method to be applied to define replicate weights. Choose between one of these: \code{JKn}, \code{dCV}, \code{bootstrap}, \code{subbootstrap}, \code{BRR}, \code{split}, \code{extrapolation}.
#' @param k A numeric value indicating the number of folds to be defined. Default is \code{k=10}. Only applies for the \code{dCV} method.
#' @param R A numeric value indicating the number of times the sample is partitioned. Default is \code{R=1}. Only applies for \code{dCV}, \code{split} or \code{extrapolation} methods.
#' @param B A numeric value indicating the number of bootstrap resamples. Default is \code{B=200}. Only applies for \code{bootstrap} and  \code{subbootstrap} methods.
#' @param dCV.sw.test A logical value indicating the method for estimating the error for \code{dCV} method. \code{FALSE}, (the default option) estimates the error for each test set and defines the cross-validated error based on the average strategy. Option \code{TRUE} estimates the cross-validated error based on the pooling strategy
#' @param train.prob A numeric value between 0 and 1, indicating the proportion of clusters (for the method \code{split}) or strata (for the method \code{extrapolation}) to be set in the training sets. Default is \code{train.prob = 0.7}. Only applies for \code{split} and \code{extrapolation} methods.
#' @param method.split A character string indicating the way in which replicate weights should be defined in the \code{split} method. Choose one of the following: \code{dCV}, \code{bootstrap} or \code{subbootstrap}. Only applies for \code{split} method.
#' @param print.rw A logical value. If \code{TRUE}, the data set with the replicate weights is saved in the output object. Default \code{print.rw=FALSE}.
#'
#' @importFrom graphics abline mtext
#' @importFrom stats as.formula coef predict runif
#' 
#' @seealso [xgboost::xgb.train()] for relevant arguments. 
#'
#' @return The output object of the function \code{wxgboost()} is an object of class \code{w.xgboost}. This object is a list containing 4 or 5 elements, depending on the value set to the argument \code{print.rw}. Below we describe the contents of these elements:
#' - `CV.iterations`: A list containing information of three elements:
#'   - `niter.range.per.R`: A numeric vector indicating all the values considered for the tuning parameter.
#'   - `k_fold.best`: A numeric value indicating the value of the tuning parameter that minimizes the average error (i.e., selected optimal tuning parameter).
#'   - `best_iteration`: 
#' - `CV.eval_log`: A list containing information of two elements:
#'   - `CV`: A numeric vector indicating the average error corresponding to each tuning parameter.
#'   - `Best_iteration`: A numeric matrix indicating the error of each test set for each tuning parameter.
#' - `final.model`: A list containing information on the fitted models: an object of class \code{xgb.Booster} found in R-\code{xgboost} library. Note that the selected model is fitted by the whole data set (and not uniquely the training sets).
#' - `predicted`: A vector of numeric predicted values from `final.model`.
#' - `data.rw`: A data frame containing the original data set and the replicate weights added to define training and test sets. Only included in the output object if \code{print.rw=TRUE}.
#' - `call`: an object containing the information about the way in which the function has been run.
#' @export
#'
#' @examples
#'  Do not run!!
#'  #to avoid only one PSU in a stratum
#'    options(survey.adjust.domain.lonely=TRUE)
#'    options(survey.lonely.psu="adjust")
#'    
#'param <- list(objective = "binary:logistic", max_depth = sample(4:8, 1),eta = runif(1, .01, .3),gamma = runif(1, 0.0, 1), 
#'              subsample = 0.5, # much smaller for large N
#'              colsample_bytree = runif(1, .5, .8)
#'              min_child_weight = sample(1:40, 1),
#'              max_delta_step = sample(1:10, 1)  # for very imbalanced case)
#'              
#' mcv <- wxgboost(data = Mydata,
#'               y = y, col.x = 1:50,
#'               cluster = "cluster", strata = "strata", weights = "weights",
#'               params = param, nrounds =10000,verbose=0,early_stopping_rounds=5,
#'               method = "dCV", k=10, R=20)
#'
#' # Or equivalently:
#' mydesign <- survey::svydesign(ids=~cluster, strata = ~strata, weights = ~weights,
#'                               nest = TRUE, data = Mydata)
#' mcv <- wxgboost(y = y, col.x = 1:50, design = mydesign,
#'               params = param, nrounds =10000,verbose=0,early_stopping_rounds=5,
#'               method = "dCV", k=10, R=20)


wxgboost <- function(data = NULL, y =NULL, col.x = NULL,missing = NA, nthread=NULL,
                   cluster = NULL, strata = NULL, weights = NULL, design = NULL,
                   params = list(), nrounds, verbose = 1, print_every_n = 1L,
                   maximize = NULL, early_stopping_rounds = 8, final.model=TRUE,
                   method = c("dCV", "JKn", "bootstrap", "subbootstrap", "BRR", "split", "extrapolation"),
                   k = 10, R = 1, B = 200, dCV.sw.test = FALSE,
                   train.prob = 0.7, method.split = c("dCV", "bootstrap", "subbootstrap"),
                   print.rw = FALSE, save_period = NULL, save_name = "wxgboost.model",
                   xgb_model = NULL, callbacks = list(), ...){

  # Stops and messages:
  if(is.null(data) & is.null(design)){stop("Information about either the data set ('data') or the sampling design ('design') needed.")}

  if(method == "split"){
    if(is.null(train.prob)){stop("Selected replicate weights method: 'split'.\nPlease, set a value between 0 and 1 for the argument 'train.prob'.")}
    if(train.prob < 0 | train.prob > 1){stop("Selected replicate weights method: 'split'.\nPlease, set a value between 0 and 1 for the argument 'train.prob'.")}
    if(length(method.split)!=1){stop("Selected replicate weights method: 'split'.\nPlease, set a valid method for the argument 'method.split'. Choose between: 'dCV', 'bootstrap' or 'subbootstrap'.")}
  }

  if(method == "extrapolation"){
    if(is.null(train.prob)){stop("Selected replicate weights method: 'extrapolation'.\nPlease, set a value between 0 and 1 for the argument 'train.prob'.")}
    if(train.prob < 0 | train.prob > 1){stop("Selected replicate weights method: 'extrapolation'.\nPlease, set a value between 0 and 1 for the argument 'train.prob'.")}
  }

  if(method %in% c("JKn", "bootstrap", "subbootstrap", "BRR")){
    if(R!=1){cat("Selected method:", method,". For this method, R = 1. Thus, the argument R =",R, "has been ignored.")}
  }

  if(method %in% c("dCV", "split", "extrapolation")){
    if(R != round(R)){stop("The argument 'R' must be an integer greater or equal to 1. R=",R," is not an integer.\nPlease, set a valid value for 'R' or skip the argument to select the default option R=1.")}
    if(R < 1){stop("The argument 'R' must be an integer greater or equal to 1. R=",R," lower than 1.\nPlease, set a valid value for 'R' or skip the argument to select the default option R=1.")}
  }

  if(method != "dCV"){
    if(!is.null(k) & k!=10){cat("Selected method:", method,". The argument k =",k, "is not needed and, hence, has been ignored.")}
  }

  if(method == "dCV"){
    if(k != round(k)){stop("The argument 'k' must be an integer. k=",k," is not an integer.\nPlease, set a valid value for 'k' or skip the argument to select the default option k=10.")}
    if(k < 1){stop("The argument 'k' must be a positive integer. k=",k," is not a positive integer.\nPlease, set a valid value for 'k' or skip the argument to select the default option k=10.")}
  }

  if(!(method %in% c("bootstrap", "subbootstrap"))){
    if(!is.null(B) & B!=200){cat("Selected method:", method,". The argument B =",B, "is not needed and, hence, has been ignored.")}
  }

  if(method %in% c("bootstrap", "subbootstrap")){
    if(B != round(B)){stop("The argument 'B' must be an integer. B=",B," is not an integer.\nPlease, set a valid value for 'B' or skip the argument to select the default option B=200.")}
    if(B < 1){stop("The argument 'B' must be a positive integer. B=",B," is not a positive integer.\nPlease, set a valid value for 'B' or skip the argument to select the default option B=200.")}
  }



  # Step 0: Notation
  if(!is.null(design)){
    cluster <- as.character(design$call$id[2])
    if(cluster == "1" || cluster == "0"){
      cluster <- NULL
    }
    strata <- as.character(design$call$strata[2])
    weights <- as.character(design$call$weights[2])
    data <- get(design$call$data)
  }


  # Step 1: Generate replicate weights based on the method
  newdata <- replicate.weights(data = data, method = method,
                               cluster = cluster, strata = strata, weights = weights,
                               k = k, R = R, B = B,
                               train.prob = train.prob, method.split = method.split,
                               rw.test = TRUE, dCV.sw.test = dCV.sw.test)


  # Step 2: Do CV to find the best iteration
  mat<- as.matrix(data[,col.x])
  
  Niter<-NULL; Niter.max<-NULL; k.fold.best.iter<-matrix(0,nrow=k,ncol = R)
  k.fold.error <- list(); 
        #attach eval.nonpara.R for testing measure
   if(params$objective == "binary:logistic"){
        eval.metric<- evalerror.bin
          # eval.metric<- eval.loss
    }else {
       eval.metric<- evalerror.reg
    }
for (r in 1:R){
    rwtraincols <- grep(paste0("r_",r,"_train"), colnames(newdata))
    rwtestcols <- grep(paste0("r_",r,"_test"),names(newdata))
    eval.log<-list();total.iter <- NULL; 
  for(k.w in 1: length(rwtraincols)){
     
      #generate supporting type data
      dtrain<- xgb.DMatrix(data = mat, label = y, weight = newdata[,rwtraincols[k.w]],missing=missing,... )
      dtest <- xgb.DMatrix(data = mat, label = y, weight = newdata[,rwtestcols[k.w]],missing = missing,...)
      watchlist <- list(train=dtrain, eval=dtest)
    
    model <- xgboost::xgb.train(params = params, data = dtrain, watchlist = watchlist, 
                               nrounds=nrounds, feval = eval.metric,
                              verbose = verbose, print_every_n = print_every_n,
                              early_stopping_rounds = early_stopping_rounds, maximize = maximize,
                              save_period = save_period, save_name = save_name,
                              xgb_model = xgb_model, callbacks = list(), ...)
       
       eval.log[[length(eval.log)+1]]<-model$evaluation_log
       names(eval.log)[[length(eval.log)]]<- names(newdata)[rwtraincols[k.w]]
       

    # if(!is.null(early_stopping_rounds)){ 
        k.fold.best.iter[k.w,r] <- model$best_iteration
        total.iter <- c(total.iter,model$niter)
      # } else{ total.iter <- model$niter
      # }
     
  }
    #find min error per r to pick best nrounds from k-fold cv
   
    min.niter<- min(total.iter)
      train.eval<- NULL
      test.eval <- NULL
    for (j in 1:k){
          train.eval <- cbind(train.eval,eval.log[[j]][1:min.niter,2])
          test.eval <- cbind(test.eval,eval.log[[j]][1:min.niter,3])
    }
    #collect min niters and corresponding iteration-wise mean for k-folds per r
      Niter<- c(Niter, min.niter); Niter.max<-c(Niter.max,max(total.iter))
      k.fold.error[[r]] <- cbind(apply(train.eval,1,mean,na.rm=TRUE),apply(test.eval,1,mean,na.rm=TRUE))
}  
    min_nstops <- min(Niter)
    train.mean.error<-NULL;test.mean.error<-NULL
    for(r in 1:R){
     train.mean.error<- cbind(train.mean.error,k.fold.error[[r]][1:min_nstops,1])
     test.mean.error<- cbind(test.mean.error,k.fold.error[[r]][1:min_nstops,2])
    }

     mean.train.error<- apply(train.mean.error,1,mean,na.rm=TRUE)
     sd.train.error <- apply(train.mean.error,1,sd,na.rm=TRUE)
     mean.test.error <- apply(test.mean.error,1,mean,na.rm=TRUE)
      sd.test.error <- apply(test.mean.error,1,sd,na.rm=TRUE)

    Eval.log <- cbind(mean.train.error,sd.train.error,mean.test.error,sd.test.error)
    best_iter <- which.min(mean.test.error)
    Eval.log<- data.frame(itr=1:min_nstops,Eval.log)
    #step 3: run the final model
    if(final.model){ 
     #generate supporting data type
    dtrain<- xgb.DMatrix(data = mat, label = y, weight = data[,weights],missing=missing,... )
   
    model <- xgboost::xgb.train(params = params, data = dtrain, 
                               nrounds=best_iter, feval = eval.metric,
                              verbose = verbose, print_every_n = print_every_n,
                              maximize = maximize,
                              save_period = save_period, save_name = save_name,
                              xgb_model = xgb_model, callbacks = list(), ...)
   
    yhat <- predict(model, newdata=mat,iterationrange = c(1,best_iter+1))
    
    if(params$objective == "binary:logistic"){
             yhat <- ifelse(yhat>0.5,1,0)
    }
   }
  result <- list()
  result$CV.iterations <- list(niter.range.per.R = rbind(Niter,Niter.max),
                               k_fold.best=k.fold.best.iter,
                               best_iteration = best_iter)
  result$CV.eval_log <- list(CV = Eval.log,
                             Best_iteration =  Eval.log[best_iter,])
  if(final.model) {result$final.model <- model
             result$predicted <- yhat}
  
  result$call <- match.call()

  if(print.rw == TRUE){result$data.rw <- newdata}

  class(result) <- "w.xgboost"

  return(result)

}
