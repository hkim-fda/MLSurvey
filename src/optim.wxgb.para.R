#' Optimal (hyper-) parameter search for weighted XGBoost (wxgboost) for complex survey data
#'
#' @description
#' A function to search for optimal (hyper-) parameters by design-based K-fold cross validation \code{dCV} for XGBoost models.
#'
#' @param y A numeric vector of response variable.
#' @param col.x A numeric vector indicating indices of the covariates or a string vector indicating these column names.
#' @param param The list of parameters.  Should include \code{objective} to be a string of either `binary:logistic` for binary classification or `reg:squarederror` for linear regression.
#'              The complete list of parameters is found on [xgboost::xgb.train()] or \url{https://xgboost.readthedocs.io/en/latest/parameter.html}.
#' @param data A data frame with information about the response variable and covariates, as well as sampling weights, strata, and cluster indicators. It could be \code{NULL} if the sampling design were added to the \code{design} argument.
#' @param nitr The number of iterations in the \code{for}-loop to search for optimal (hyper-) parameters.  Default is `100`.
#' @param nfolds The number of folds to be defined for \code{dCV} method. Default is \code{k=10}.
#' @param R The number of times the sample is partitioned for the \code{dCV} method. Default is \code{R=1}.
#' @param nRounds The maximum number of boosting iterations.
#' @param nstop \code{NULL} does not trigger the early stopping function. Setting to an integer k will stop training with a validation set unless the performance improve for k rounds.  Setting this parameter engages the `cb.early.stop` callback.
#' @param maximize \code{nstop} is set, then this parameter must be set as well. When it is TRUE, it means the larger the evaluation score the better. This parameter is passed to the cb.early.stop callback.
#' @param cluster A character string indicating the name of cluster identifiers. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.
#' @param strata A character string indicating the name of strata identifiers. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.
#' @param weights A character string indicating the name of sampling weights. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.
#' @param design An object of class \code{survey.design} generated by \code{survey::svydesign()}. It could be \code{NULL} if information about \code{cluster}, \code{strata}, \code{weights} and \code{data} were given.
#' @param print_every_n Print each n-th iteration evaluation messages when verbose>0. Default is `50L` printing every 50th iteration message. This parameter is passed to the `cb.print.evaluation` callback.
#' 
#' @seealso [xgboost::xgb.train()] for relevant arguments and return values in detail. 
#' @seealso [wxgboost()] for detailed information on implementation.
#' 
#' @return   A list of return values for optimal (hyper-)parameters as follows:
#' - `nrounds` : The best iteration number having the best evaluation metric value
#' - `snumber` : A seed number for \code{set.seed()} yielding the optimal (hyper-)parameters.
#' - `params`  : A list of optimal (hyper-) parameters selected among `nitr` models implemented by `wxgboost()` with `dCV` on every iteration.  
#' @export
#' 
#' @examples
#' # Do not run!!
#'  ## Set user defined (hyper-)parameters: 
#'  param <- list(objective = "binary:logistic",
#'                  max_depth = sample(4:8, 1),
#'                   eta = runif(1, .01, .3),
#'                   gamma = runif(1, 0.0, 1), 
#'                   subsample = 0.5, # much smaller for large N
#'                   colsample_bytree = runif(1, .5, .8)
#'                   min_child_weight = sample(1:40, 1),
#'                   max_delta_step = sample(1:10, 1) ) # for very imbalanced case 
#' 
#'   ## search for optimal parameters
#'  Mydesign <- survey::svydesign(ids=~cluster, strata = ~strata, weights = ~weights,
#'                               nest = TRUE, data = Mydata)
#'  opt.par<- optim.wxgb.para(Mydata$y,2:61,param,Mydata,100,5,10,10000,8,design=Mydesign,print_every_n=250L)
#'  
#'  ## fitting the optimal model
#'  set.seed(opt.par$snumber)
#'  
#'  ### The data needs to be a sparse matrix without intercept.
#'  Mat<- sparse.model.matrix(y~. ,data = MyData_unweighted)[,-1] #dropping intercept
#'  wtData <- xgb.DMatrix(data = Mat,label=Mydata$y,weight=Mydata$weights)
#'  
#'  wxgb<-xgb.train(data=wtData, params=opt.par$params, nrounds=opt.para$nrounds,feval = evalerror.bin)
#'
#'   
optim.wxgb.para<-function(y,col.x,param=list(),data=NULL, nitr=100,nfolds=10,R=1,nRounds=10000,nstop=5,
                          missing = NA, nthread=NULL,maximize=FALSE,cluster = NULL, strata = NULL, weights = NULL, 
                          design = NULL,print_every_n=50L,...){
  
  
                 for (iter in 1:nitr) {
                    seed.number = sample.int(10000,1)
                    set.seed(seed.number)
                    wxgb<-wxgboost(data = data, y =y, col.x = col.x,missing = missing, nthread=nthread,
                                   cluster = cluster, strata = strata, weights = NULL, design = NULL,
                                   params = param, nrounds=nRounds, verbose = 0, print_every_n = print_every_n,
                                   early_stopping_rounds = nstop, final.model=FALSE,
                                   method = "dCV",maximize=maximize,
                                   k = nfolds, R = R, dCV.sw.test = FALSE,
                                   print.rw = FALSE, save_period = NULL, save_name = "wxgboost.model",
                                   xgb_model = NULL, callbacks = list(), ...)
                    
                    min_weight.error = min(wxgb$CV.eval_log$CV$mean.test.error)
                    best_eval = 100
                    if (min_weight.error < best_eval) {
                             best_eval = min_weight.error
                             best_iter_num = wxgb$CV.iterations$best_iteration # having the best evaluation metric value
                             best_seednumber = seed.number
                             best_param = param
            }
        }

   return(list(nrounds=best_iter_num,snumber=best_seednumber,params = best_param))
 }
