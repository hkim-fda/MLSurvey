#' Weighted Elastic Net prediction models for complex survey data
#'
#'@description This function fits Elastic Net prediction (linear or logistic) models to complex survey data, using sampling weights in the estimation process 
#'            and selects the lambda that minimizes the error based on different replicating weights methods.
#'
#' @param data A data frame with information about the response variable and covariates, as well as sampling weights, strata, and cluster indicators. It could be \code{NULL} if the sampling design were replaced in the \code{design} argument.
#' @param col.y A numeric value indicating the index of the response variable or a string of the response variable name .
#' @param col.x A numeric vector indicating indices of covariates or a vector of strings indicating these column names.
#' @param cluster A character string indicating the name of the column with cluster identifiers. It could be \code{NULL} if the sampling design were indicated in the \code{design} argument.
#' @param strata A string of a strata identifier. It could be \code{NULL} if the sampling design were plugged in the \code{design} argument.
#' @param weights A string of the name of sampling weights. It could be \code{NULL} if the sampling design were included in the \code{design} argument.
#' @param design An object of class \code{survey.design} generated by \code{survey::svydesign()}. It could be \code{NULL} if information about \code{cluster}, \code{strata}, \code{weights} and \code{data} were given.
#' @param family A string indicating a \code{glm} family object for fitting weighted elastic net models. Choose between \code{gaussian} (to fit linear models) or \code{binomial} (for logistic models).
#' @param lambda.grid A vector of a grid for penalization parameters. The default option is \code{lambda.grid = NULL} generated by the function \code{glmnet::glmnet()}.
#' @param alpha A numeric value of the elastic net mixing parameter 0 \deqn{\ge \alpha \le 1}.  \code{alpha = 1} for the LASSO penalty and \code{alpha = 0} for the Ridge penalty.
#' @param method A string indicating the method of replicate weights. Choose between one of these: \code{JKn}, \code{dCV}, \code{bootstrap}, \code{subbootstrap}, \code{BRR}, \code{split}, \code{extrapolation}.
#' @param k A numeric value defining the number of folds for the \code{dCV} method. Default is \code{k=10}.
#' @param R A numeric value indicating the number of times the sample is partitioned for \code{dCV}, \code{split} or \code{extrapolation} methods. Default is \code{R=1}.
#' @param B A numeric value indicating the number of bootstrap resamples for \code{bootstrap} and  \code{subbootstrap} methods. Default is \code{B=200}.
#' @param dCV.sw.test A logical value indicating the method for estimating the error for \code{dCV} method. \code{FALSE}, (the default option) estimates the error for each test set and defines the cross-validated error based on the average strategy. 
#'                    Option \code{TRUE} estimates the cross-validated error based on the pooling strategy.
#' @param train.prob A numeric value between 0 and 1, the proportion of clusters (for the method \code{split}) or strata (for the method \code{extrapolation}) to be set in the training sets. Default is \code{train.prob = 0.7}.
#' @param method.split A character string indicating how to define replicate weights in the \code{split} method. Choose one of the following: \code{dCV}, \code{bootstrap} or \code{subbootstrap}. 
#' @param print.rw A logical value. If \code{TRUE}, the data set with the replicate weights is saved in the output object. Default \code{print.rw=FALSE}.
#'
#' @importFrom graphics abline mtext
#' @importFrom stats as.formula coef predict runif
#' 
#' @seealso [wlasso::wlasso()] for detailed arguments.
#' @seealso [glmnet::glmnet()] for detailed arguments
#'
#' @return The output object of the function \code{welnet()} is an object of class \code{w.elnet}. This object is a list containing 4 or 5 elements, depending on the value set to the argument \code{print.rw}. Below we describe the contents of these elements:
#' - `lambda`: A list containing information of two elements:
#'   - `grid`: A vector of all grid values for the tuning parameter.
#'   - `min`: A numeric value of an optimal tuning parameter minimizing the average error.
#' - `error`: A list containing information of two elements:
#'   - `average`: A numeric vector indicating the average error for each tuning parameter in \code{lambda$grid}.
#'   - `all`: A numeric matrix indicating the error of each test set for each tuning parameter.
#' - `model`: A list containing information of two elements in relation to the fitted models. Note that all these models are fitted with the whole data set (and not uniquely the training sets).
#'   - `grid`: A list with the information on models fitted by each tuning parameter in the \code{lambda$grid}.
#'     - `a0`: A numeric vector of model intercepts across the whole grid of tuning parameters (hence, of the same length as \code{lambda$grid}).
#'     - `beta`: A matrix of regression coefficients for all the covariates across the whole grid of tuning parameters (the dimension of the matrix is the size of covariates by the length of \code{lambda$grid}).
#'     - `df`: A numeric vector of the degrees of freedom (i.e., the number of coefficients different from zero) across the whole grid of tuning parameters  (hence, of the same length as \code{lambda$grid}).
#'   - `final_model`: An object with S3 class \code{glmnet} found in r-\code{glmnet}.  A list with the information on the final model fitted by the optimal tuning parameter, \code{lambda$min}. 
#'            (i.e., the optimal tuning parameter selected among the elements in \code{lambda$grid}).  @seealso [glmnet::glmnet()] for detailed information on return values.    
#' - `data.rw`: A data frame containing the original data set and the replicate weights added to define training and test sets. Only included in the output object if \code{print.rw=TRUE}.
#' - `call`: The call that executes this function producing the object of \code{w.elnet}.
#'
#' 
#' 
#'
#' @examples
#' # Do not run!
#' # alpha can be selected based on `glmnet::glmnet()`:
#' alpha <- c(0.000, 0.001, 0.008, 0.027, 0.064, 0.125, 0.216, 0.343, 0.512, 0.729, 1.000)
#' en.dcv <- welnet(data = Mydata,
#'               col.y = "y", col.x = 1:50,
#'               family = "binomial", alpha = 0.729,
#'               cluster = "cluster", strata = "strata", weights = "weights",
#'               method = "dCV", k=10, R=20)
#'
#' # Or equivalently:
#' Mydesign <- survey::svydesign(ids=~cluster, strata = ~strata, weights = ~weights,
#'                               nest = TRUE, data = Mydata)
#' en.dcv <- welnet(col.y = "y", col.x = 1:50, design = Mydesign,
#'               family = "binomial", alpha = alpha[10],
#'               method = "dCV", k=10, R=20)
#'               
#'@export
# source("error.f.R")
welnet <- function(data = NULL, col.y = NULL, col.x = NULL,
                   cluster = NULL, strata = NULL, weights = NULL, design = NULL,
                   family = c("gaussian", "binomial"),
                   lambda.grid = NULL, alpha = 1,
                   method = c("dCV", "JKn", "bootstrap", "subbootstrap", "BRR", "split", "extrapolation"),
                   k = 10, R = 1, B = 200,
                   dCV.sw.test = FALSE,
                   train.prob = 0.7, method.split = c("dCV", "bootstrap", "subbootstrap"),
                   print.rw = FALSE){

  # Stops and messages:
  if(is.null(data) & is.null(design)){stop("Information about either the data set ('data') or the sampling design ('design') needed.")}

  if(method == "split"){
    if(is.null(train.prob)){stop("Selected replicate weights method: 'split'.\nPlease, set a value between 0 and 1 for the argument 'train.prob'.")}
    if(train.prob < 0 | train.prob > 1){stop("Selected replicate weights method: 'split'.\nPlease, set a value between 0 and 1 for the argument 'train.prob'.")}
    if(length(method.split)!=1){stop("Selected replicate weights method: 'split'.\nPlease, set a valid method for the argument 'method.split'. Choose between: 'dCV', 'bootstrap' or 'subbootstrap'.")}
  }

  if(method == "extrapolation"){
    if(is.null(train.prob)){stop("Selected replicate weights method: 'extrapolation'.\nPlease, set a value between 0 and 1 for the argument 'train.prob'.")}
    if(train.prob < 0 | train.prob > 1){stop("Selected replicate weights method: 'extrapolation'.\nPlease, set a value between 0 and 1 for the argument 'train.prob'.")}
  }

  if(method %in% c("JKn", "bootstrap", "subbootstrap", "BRR")){
    if(R!=1){cat("Selected method:", method,". For this method, R = 1. Thus, the argument R =",R, "has been ignored.")}
  }

  if(method %in% c("dCV", "split", "extrapolation")){
    if(R != round(R)){stop("The argument 'R' must be an integer greater or equal to 1. R=",R," is not an integer.\nPlease, set a valid value for 'R' or skip the argument to select the default option R=1.")}
    if(R < 1){stop("The argument 'R' must be an integer greater or equal to 1. R=",R," lower than 1.\nPlease, set a valid value for 'R' or skip the argument to select the default option R=1.")}
  }

  if(method != "dCV"){
    if(!is.null(k) & k!=10){cat("Selected method:", method,". The argument k =",k, "is not needed and, hence, has been ignored.")}
  }

  if(method == "dCV"){
    if(k != round(k)){stop("The argument 'k' must be an integer. k=",k," is not an integer.\nPlease, set a valid value for 'k' or skip the argument to select the default option k=10.")}
    if(k < 1){stop("The argument 'k' must be a positive integer. k=",k," is not a positive integer.\nPlease, set a valid value for 'k' or skip the argument to select the default option k=10.")}
  }

  if(!(method %in% c("bootstrap", "subbootstrap"))){
    if(!is.null(B) & B!=200){cat("Selected method:", method,". The argument B =",B, "is not needed and, hence, has been ignored.")}
  }

  if(method %in% c("bootstrap", "subbootstrap")){
    if(B != round(B)){stop("The argument 'B' must be an integer. B=",B," is not an integer.\nPlease, set a valid value for 'B' or skip the argument to select the default option B=200.")}
    if(B < 1){stop("The argument 'B' must be a positive integer. B=",B," is not a positive integer.\nPlease, set a valid value for 'B' or skip the argument to select the default option B=200.")}
  }



  # Step 0: Notation
  if(!is.null(design)){
    cluster <- as.character(design$call$id[2])
    if(cluster == "1" || cluster == "0"){
      cluster <- NULL
    }
    strata <- as.character(design$call$strata[2])
    weights <- as.character(design$call$weights[2])
    data <- get(design$call$data)
  }


  # Step 1: Generate replicate weights based on the method
  newdata <- replicate.weights(data = data, method = method,
                               cluster = cluster, strata = strata, weights = weights,
                               k = k, R = R, B = B,
                               train.prob = train.prob, method.split = method.split,
                               rw.test = TRUE, dCV.sw.test = dCV.sw.test)


  # Step 2: if is.null(lambda.grid), then initialize it
  if(is.null(lambda.grid)){
    model.orig <- glmnet::glmnet(y = as.numeric(newdata[,col.y]),
                                 x = as.matrix(newdata[,col.x]),
                                 weights = as.numeric(newdata[,weights]), 
                                 alpha = alpha, 
                                 family = family)
    lambda.grid <- model.orig$lambda
  } else {
    model.orig <- glmnet::glmnet(y = as.numeric(newdata[,col.y]),
                                 x = as.matrix(newdata[,col.x]),
                                 weights = as.numeric(newdata[,weights]),
                                 family = family, alpha = alpha,
                                 lambda = lambda.grid)
  }

  # Step 3: Fit the training models and estimate yhat for units in the sample
  rwtraincols <- grep("_train", colnames(newdata))
  l.yhat <- list()
  # alpha <- c(0.000, 0.001, 0.008, 0.027, 0.064, 0.125, 0.216, 0.343, 0.512, 0.729, 1.000)
  for(col.w in rwtraincols){

    model <- glmnet::glmnet(y = as.numeric(newdata[,col.y]),
                            x = as.matrix(newdata[,col.x]),
                            weights = as.numeric(newdata[,col.w]),
                            lambda = lambda.grid, alpha = alpha,
                            family = family)

    # Sample yhat
    yhat <- predict(model, newx=as.matrix(newdata[,col.x]), type = "response")
    l.yhat[[length(l.yhat) + 1]] <- yhat
    names(l.yhat)[[length(l.yhat)]] <- paste0("yhat_", colnames(newdata)[col.w])

  }

  # Step 4: estimate the error in the test sets
  error <- error.f(data = newdata, l.yhat = l.yhat,
                   method = method, cv.error.ind = dCV.sw.test,
                   R = R, k = k, B = B,
                   col.y = col.y, family = family, weights = weights)
  mean.error <- apply(error, 2, mean)

  lambda.min <- lambda.grid[which.min(mean.error)]

  model <- glmnet::glmnet(y = data[,col.y],
                          x = as.matrix(data[,col.x]),
                          weights = data[,weights],
                          lambda = lambda.min, alpha = alpha,
                          family = family)
  
  result <- list()
  result$lambda <- list(grid = lambda.grid,
                        min = lambda.min)          
  result$error <- list(average = mean.error,
                       all = error)
  result$model <- list()
  result$model$grid <- list(a0 = model.orig$a0,
                            beta = model.orig$beta,
                            df = model.orig$df)
  result$model$final_model <- model
  result$call <- match.call()

  if(print.rw == TRUE){result$data.rw <- newdata}

  class(result) <- "w.elnet"

  return(result)

}
